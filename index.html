<!DOCTYPE html>
<html lang="en">
<head>
    <title>Sergey Tulyakov</title>
    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet"/>
    <link href="css/bootstrap-social.css" rel="stylesheet"/>
    <link href="css/custom.css" rel="stylesheet"/>
    <!-- Custom CSS -->
    <link href="css/modern-business.css" rel="stylesheet"/>
    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

    <script src="js/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script>

    <script src="https://www.googletagmanager.com/gtag/js?id=UA-115492969-1"></script>
    <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'UA-115492969-1');

    </script>
</head>
<body>
<!-- Page Content -->
<div class="container">
    <!-- Marketing Icons Section -->
    <div class="row">
        <div class="col-md-4 col-sm-4">
            <img class="img-responsive img-rounded" src="images/sergey-2021.jpg" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <h1 class="page-header first-header">Sergey Tulyakov</h1>
            <p>
                Welcome! I am a Principal Research Scientist heading the <a href="https://research.snap.com/team/category/creative-vision">Creative Vision</a> team at <a href="https://research.snap.com/">Snap Research</a>. My work
                focuses on creating methods for manipulating the world via computer vision and machine learning. This
                includes 2D and 3D methods for photorealistic object manipulation and animation, video synthesis, prediction
                and retargeting. My work has been published as 30+ top conference papers, journals and patents resulting
                in multiple tech transfers, including Snapchat Pet Tracking and Real-time Neural Lenses (gender swap,
                baby face).
            </p>

	    <p>
	    <a class="btn btn-primary" data-toggle="collapse" href="#past-experience" role="button" aria-expanded="false" aria-controls="past-experience">
    		View my past experience
	    </a>
	    </p>
	    <div class="collapse" id="past-experience">

            <p> Before joining Snap Inc., I built the real-time face tracking and reconstruction engine behind <a
                    href="https://www.facebook.com/facify/videos/426807287711138/">facify.me</a>. 2016, I was a research
                intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/">Microsoft
                    Research</a>, Cambridge, UK, and worked with <a href="http://www.nowozin.net/sebastian/">Sebastian
                    Nowozin</a> and <a href="https://www.microsoft.com/en-us/research/people/awf/">Andrew Fitzgibbon</a>
                on improving generative models.
                In 2017, I interned at NVIDIA with <a href="http://mingyuliu.net/">Ming-Yu Liu</a>, <a
                        href="http://xiaodongyang.org/">Xiaodong Yang</a> and <a href="http://jankautz.com/">Jan
                    Kautz</a> and worked on video generation. </p>

            <p>I am a PhD graduate from the University of Trento, Italy, where I worked with <a
                    href="http://disi.unitn.it/~sebe/">Nicu Sebe</a> at the Multimedia and Human Understanding Group
                <a href="http://mhug.disi.unitn.it/">MHUG</a>. Before joining the MHUG group, I worked as a software
                engineer for six years.</p>

	    </div>

            <p>I am looking for prospective interns and collaborators. If you are interested send me an email.</p>
            <p>Media coverage:</p>
            <ul>
                <li>Pet Tracking: <a href="https://www.thesun.co.uk/tech/7478208/snapchat-cat-filters-how-to-lenses/">The
                    Sun</a>, <a
                        href="https://www.theverge.com/2018/12/26/18156183/snapchat-dog-lenses-filter-christmas">The
                    Verge</a>, <a
                        href="https://variety.com/2019/digital/news/secret-life-of-pets-2-snapchat-ar-lens-cat-dog-1203154288/">Variety</a>,
                    <a href="https://www.cnet.com/news/snapchat-unleashes-lenses-for-your-pup/">CNet</a> and others
                </li>
                <li>Real-time Neural Lenses: <a
                        href="https://www.cnet.com/how-to/want-to-try-that-gender-face-swap-thing-everyones-doing-heres-how/">CNet</a>,
                    <a href="https://www.theverge.com/2019/6/11/18661623/snapchat-gender-swap-filter-police-underage-hookup">BoredPanda</a>,
                    <a href="https://www.theverge.com/2019/6/11/18661623/snapchat-gender-swap-filter-police-underage-hookup">Newsweek</a>,
                    <a href="https://www.telegraph.co.uk/technology/2019/07/23/baby-face-gender-swap-filters-propel-snapchat-strongest-growth/">The
                        Telegraph</a> and others
                </li>
                <li>Time Machine Lens: <a
                        href="https://www.elitedaily.com/p/snapchats-new-time-machine-lens-combines-your-2-of-your-favorite-filters-19371910/">elite daily</a>,
                    <a href="https://www.engadget.com/2019-11-21-snapchats-lens-time-machine-aging-selfie.html">Engadget</a>,
                     and others
                </li>
                <li>Image Animation: <a
                        href="https://www.youtube.com/watch?v=mUfJOQKdtAk">Two-minute papers</a>,
                    <a href="https://www.vice.com/en_us/article/g5xagy/this-open-source-program-deepfakes-you-during-zoom-meetings-in-real-time">Vice</a>,
                    <a href="https://thenextweb.com/neural/2020/04/21/watch-fake-elon-musk-zoom-bombs-meeting-using-real-time-deepfake-ai">The Next Web</a>,
                     and others
                </li>
            </ul>
            <p>Contact:
                <a href="mailto:stulyakov@snap.com">stulyakov@snap.com</a></p>
            <br/>
            <a class="btn btn-social-icon btn-twitter" href="https://twitter.com/SergeyTulyakov"><i
                    class="fa fa-twitter"></i></a>
            <a class="btn btn-social-icon btn-facebook" href="https://www.facebook.com/s.tulyakov"><i
                    class="fa fa-facebook"></i></a>
            <a class="btn btn-social-icon btn-github" href="https://github.com/sergeytulyakov"><i
                    class="fa fa-github"></i></a>
            <a class="btn btn-social-icon btn-linkedin" href="https://www.linkedin.com/in/sergeytulyakov"><i
                    class="fa fa-linkedin"></i></a>
            <a class="btn btn-primary" href="data/Sergey_Tulyakov_CV.pdf">CV</a>
            <a class="btn btn-primary" href="https://scholar.google.it/citations?user=3H76Zg0AAAAJ">Google Scholar</a></div>
    

    </div>
    <div class="row">
	<div class="col-lg-12 col-md-8 col-sm-12">
            <h3 class="page-header">Research</h3>
        </div>
        <div class="col-md-12">
		<p>The ultimate goal of the <a href="https://research.snap.com/team/category/creative-vision">Creative Vision</a> group is to develop cutting edge computer vision, machine
                learning, and graphics technology to unlock creativity of users and creators in a variety of products at
                Snap. Our work enables machines to see, understand and manipulate the world. We focus on the following
                broad directions:
            <ul>
                <li><b>Understanding Humans:</b> Human digitization (segmentation, 3D pose and geometry of face, body,
                    hand, clothes)
                </li>
                <li><b>Understanding the World:</b> Digitization of the world and objects in it (recognition,
                    segmentation, 3D reconstruction, scene understanding)
                </li>
                <li><b>Tools for Creativity:</b> Arbitrary AR manipulation of the world, including style transfer for
                    shape and texture of faces, objects and scenes, adding and removing objects from the scene
                </li>
                <li><b>Efficient Algorithms and Architectures:</b> Technologies for faster, smaller models and more
                    accurate inference
                </li>
            </ul>
            We are hiring! Feel free to reach out to me if you are interested to know more or apply <a
                href="https://wd1.myworkdaysite.com/recruiting/snapchat/snap/job/Los-Angeles-California/Research-Scientist_R0009063">here</a>.

        </div>
    </div>

    <div class="row">
        <div class="col-lg-12">
            <h3 class="page-header">Work Experience</h3>
            <div class="col-md-3">
                <p>July 2017 - Present:</p>
            </div>
            <div class="col-md-8">
                <p>Principal Research Scientist at Snap Research, Santa Monica, CA</p>
            </div>
            <div class="col-md-3">
                <p>Jan 2017 - Apr 2017:</p>
            </div>
            <div class="col-md-8">
                <p>Research Intern at NVIDIA Research, Santa Clara, CA</p>
            </div>
            <div class="col-md-3">
                <p>Aug 2016 - Nov 2016:</p>
            </div>
            <div class="col-md-8">
                <p>Research Intern at Microsoft Research, Cambridge, UK</p>
            </div>
            <div class="col-md-3">
                <p>July 2010 - Sept 2012:</p>
            </div>
            <div class="col-md-8">
                <p> Team and Project Lead at HiQo-Solutions, Minsk, Belarus </p>
            </div>
        </div>
        <div class="col-md-12">
            <div class="col-md-3">
                <p>June 2006 - June 2010:</p>
            </div>
            <div class="col-md-8">
                <p>Software Engineer at Todes Ltd, Minsk, Belarus </p>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-lg-12">
            <h3 class="page-header">Education</h3>
            <div class="col-md-3">
                <p>Nov 2012 - Apr 2017:</p>
            </div>
            <div class="col-md-8">
                <p>PhD Canditate at The Univesity of Trento, Italy</p>
            </div>
        </div>
        <div class="col-md-12">
            <div class="col-md-3">
                <p>Sept 2014 - Feb 2015:</p>
            </div>
            <div class="col-md-8">
                <p>Research intern at the Robotics Institute, Carnegie-Mellon University</p>
            </div>
        </div>
        <div class="col-md-12">
            <div class="col-md-3">
                <p>Sept 2009 - July 2010:</p>
            </div>
            <div class="col-md-8">
                <p>MSc at the Belarusian State University of Intormatics and Radioelectronics</p>
            </div>
        </div>
        <div class="col-md-12">
            <div class="col-md-3">
                <p>Sept 2004 - July 2009:</p>
            </div>
            <div class="col-md-8">
                <p>B.Eng at the Belarusian State University of Intormatics and Radioelectronics</p>
            </div>
        </div>
    </div>


    <div class="row">
        <div class="col-lg-12">
            <h3 class="page-header">Teaching</h3>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Teaching Computers to Imagine with Deep Generative Models</p>
                </strong></p>
                <p>Sergey Tulyakov, Stéphane Lathuilière</p>
                <p>University of Trento, Italy. November 19-26, 2019</p>
                <a href="teaching/teaching-to-imagine.html" class="btn btn-primary">Course material</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Unlocking Creativity with Computer Vision: Representations for Animation, Stylization and Manipulation</p>
                </strong></p>
                <p>June 20, CVPR Virtual 2021</p>
                <a href="https://snap-research.github.io/representations-for-creativity/" class="btn btn-primary">Tutorial webpage</a>
            </div>
        </div>
    </div>
    <hr/>
    <div class="row">
        <div class="col-lg-12">
            <h3 class="page-header">Publications</h3>

        </div>
    </div>

    <div class="row">
	<div class="col-md-3 col-sm-3">
	    <video class="img-responsive" loop autoplay muted>
			<source src="images/epigraf-teaser.mp4" type="video/mp4">
		</video>

	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>EpiGRAF: Rethinking training of 3D GANs</p>
		</strong></p>
		<p>Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, Peter Wonka</p>
		<p><em>Neural Information Processing Systems, 2022</em></p>
		<a href="https://universome.github.io/epigraf" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2206.10535" class="btn btn-primary">Paper</a>
		<a href="https://github.com/universome/epigraf" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>
   

    <div class="row">
	<div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/efficient-former.png" alt=""/>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>EfficientFormer: Vision Transformers at MobileNet Speed</p>
		</strong></p>
		<p>Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren</p>
		<p><em>Neural Information Processing Systems, 2022</em></p>
		<a href="https://arxiv.org/abs/2206.01191" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/EfficientFormer" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>

    <div class="row">
	<div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/r2l.gif" alt=""/>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis</p>
		</strong></p>
		<p>Huan Wang, Jian Ren, Zeng Huang, Menglei Chai, Kyle Olszewski, Yun Fu, Sergey Tulyakov</p>
		<p><em>European Conference on Computer Vision, 2022</em></p>
		<a href="https://snap-research.github.io/R2L/" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2203.17261" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/R2L" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>

    <div class="row">
	<div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/d2m.png" alt=""/>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>Quantized GAN for Complex Music Generation from Dance Videos</p>
		</strong></p>
		<p>Ye Zhu, Kyle Olszewski, Yu Wu, Panos Achlioptas, Menglei Chai, Yan Yan, Sergey Tulyakov</p>
		<p><em>European Conference on Computer Vision, 2022</em></p>
		<a href="https://arxiv.org/abs/2204.00604" class="btn btn-primary">Paper</a>
		<a href="https://github.com/L-YeZhu/D2M-GAN" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>


    <div class="row">
	<div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/edit3d.jpg" alt=""/>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>Cross-Modal 3D Shape Generation and Manipulation</p>
		</strong></p>
		<p>Zezhou Cheng, Menglei Chai, Jian Ren, Hsin-Ying Lee, Kyle Olszewski, Zeng Huang, Subhransu Maji, Sergey Tulyakov</p>
		<p><em>European Conference on Computer Vision, 2022</em></p>
		<a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2207.11795" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/edit3d" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>
   
    <div class="row">
	<div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/neroic.mp4" type="video/mp4">
		</video>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>NeROIC: Neural Rendering of Objects from Online Image Collections</p>
		</strong></p>
		<p>Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, Sergey Tulyakov</p>
		<p><em>ACM Transactions on Graphics (SIGGRAPH), 2022</em></p>
		<a href="https://formyfamily.github.io/NeROIC/" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2201.02533" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/NeROIC" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>

    <div class="row">
	<div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/pe.mp4" type="video/mp4">
		</video>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>Playable Environments: Video Manipulation in Space and Time</p>
		</strong></p>
		<p>Willi Menapace, Stéphane Lathuilière, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, Elisa Ricci</p>
		<p><em>Computer Vision and Pattern Recognition, 2022</em></p>
		<a href="https://willi-menapace.github.io/playable-environments-website/" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2203.01914" class="btn btn-primary">Paper</a>
		<a href="https://github.com/willi-menapace/PlayableEnvironments" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>

    <div class="row">
	<div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/mmvid.mp4" type="video/mp4">
		</video>
	</div>
	<div class="col-md-8 col-sm-8">
	    <div>
		<p><strong>
		<p>Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning</p>
		</strong></p>
		<p>Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov</p>
		<p><em>Computer Vision and Pattern Recognition, 2022</em></p>
		<a href="https://snap-research.github.io/MMVID/" class="btn btn-primary">Project page</a>
		<a href="https://arxiv.org/abs/2203.02573" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/MMVID" class="btn btn-primary">Code</a>
	    </div>
	</div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/stylegan-v.mp4" type="video/mp4">
		</video>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2</p>
                </strong></p>
                <p>Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny</p>
                <p><em>Computer Vision and Pattern Recognition, 2022</em></p>
                <a href="https://universome.github.io/stylegan-v" class="btn btn-primary">Project page</a>
                <a href="https://arxiv.org/abs/2112.14683" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>


    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/inout.png" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>In&Out: Diverse Image Outpainting via GAN Inversion</p>
                </strong></p>
                <p>Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Ming-Hsuan Yang</p>
                <p><em>Computer Vision and Pattern Recognition, 2022</em></p>
                <a href="https://yccyenchicheng.github.io/InOut" class="btn btn-primary">Project page</a>
                <a href="https://arxiv.org/abs/2104.00675" class="btn btn-primary">Paper</a>
		<a href="https://github.com/yccyenchicheng/InOut" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/f8net.png" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Oral] F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization</p>
                </strong></p>
                <p>Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov</p>
                <p><em>International Conference on Learning Representations, 2022</em></p>
                <a href="https://openreview.net/forum?id=_CfpJazzXT2" class="btn btn-primary">Openreview</a>
                <a href="https://arxiv.org/abs/2202.05239v1" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/f8net" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/infinityGAN.png" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>InfinityGAN: Towards Infinite-Resolution Image Synthesis</p>
                </strong></p>
                <p>Chieh Hubert Lin, Hsin-Ying Lee, Yen-Chi Cheng, Sergey Tulyakov, Ming-Hsuan Yang</p>
                <p><em>International Conference on Learning Representations, 2022</em></p>
                <a href="https://hubert0527.github.io/infinityGAN/" class="btn btn-primary">Project page</a>
                <a href="https://arxiv.org/abs/2104.00675" class="btn btn-primary">Paper</a>
		<a href="https://github.com/hubert0527/infinityGAN" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/flow-guided.gif" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Flow Guided Transformable Bottleneck Networks for Motion Retargeting</p>
                </strong></p>
                <p>Jian Ren, Menglei Chai, Oliver Woodford, Kyle Olszewski, Sergey Tulyakov</p>
                <p><em>Computer Vision and Pattern Recognition, 2021</em></p>
                <a href="https://arxiv.org/abs/2106.07771" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/motion-representations-s.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Motion Representations for Articulated Animation</p>
                </strong></p>
                <p>Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, Sergey Tulyakov</p>
                <p><em>Computer Vision and Pattern Recognition, 2021</em></p>
		<a href="https://snap-research.github.io/articulated-animation/" class="btn btn-primary">Project page</a>
                <a href="https://arxiv.org/abs/2104.11280" class="btn btn-primary">Paper</a>
		<a href="https://github.com/snap-research/articulated-animation" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/cat.png" alt=""/>
        </div>
        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Teachers Do More Than Teach: Compressing Image-to-Image Models</p>
                </strong></p>Qing Jin, Jian Ren, Oliver J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, Sergey Tulyakov
                <p></p>
                <p><em>Computer Vision and Pattern Recognition, 2021</em></p>
                <a href="https://arxiv.org/abs/2103.03467" class="btn btn-primary">Paper</a>
                <a href="https://github.com/snap-research/CAT" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/pvg.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Oral] Playable Video Generation</p>
                </strong></p>
                <p>Willi Menapace, Stéphane Lathuilière, Sergey Tulyakov, Aliaksandr Siarohin, Elisa Ricci</p>
                <p><em>Computer Vision and Pattern Recognition, 2021</em></p>
		<a href="https://willi-menapace.github.io/playable-video-generation-website/" class="btn btn-primary">Project page</a>
                <a href="https://arxiv.org/abs/2101.12195" class="btn btn-primary">Paper</a>
		<a href="https://github.com/willi-menapace/PlayableVideoGeneration" class="btn btn-primary">Code</a>
		<a href="https://willi-menapace.github.io/playable-video-generation-website/play.html" class="btn btn-primary">Demo</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/mocogan-hd.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Spotlight] A Good Image Generator Is What You Need for High-Resolution Video Synthesis</p>
                </strong></p>
                <p>Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov</p>
                <p><em>International Conference on Learning Representations, 2021</em></p>
		<a href="https://bluer555.github.io/MoCoGAN-HD/" class="btn btn-primary">Project page</a>
                <a href="https://openreview.net/forum?id=6puCSjH3hwA" class="btn btn-primary">Paper</a>
                <a href="https://github.com/snap-research/MoCoGAN-HD" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/michigan.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>MichiGAN: Multi-Input-Conditioned Hair Image Generation for Portrait Editing</p>
                </strong></p>
                <p>Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, Nenghai Yu</p>
                <p><em>ACM Transactions on Grahpics (SIGGRAPH), 2020</em></p>
                <a href="https://arxiv.org/abs/2010.16417" class="btn btn-primary">Paper</a>
                <a href="https://github.com/tzt101/MichiGAN" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/patch-based.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Interactive video stylization using few-shot patch-based training</p>
		<p>Best in Show Award at SIGGRAPH Real-Time Live!</p>
                </strong></p>Ondrej Texler, David Futschik, Michal Kucera, Ondrej Jamriska, Sarka Sochorova, Menglei Chai, Sergey Tulyakov, Daniel Sykora
                <p></p>
                <p><em>ACM Transactions on Grahpics (SIGGRAPH), 2020</em></p>
                <a href="https://arxiv.org/abs/2004.14489" class="btn btn-primary">Paper</a>
                <a href="https://github.com/OndrejTexler/Few-Shot-Patch-Based-Training" class="btn btn-primary">Code</a>
                <a href="https://ondrejtexler.github.io/patch-based_training/" class="btn btn-primary">Project page</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
		<video class="img-responsive" loop autoplay muted>
			<source src="images/hair-rendering.mp4" type="video/mp4">
		</video>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Neural Hair Rendering</p>
                </strong></p>
                <p>Menglei Chai, Jian Ren, Sergey Tulyakov</p>
                <p><em>European Conference on Computer Vision, 2020</em></p>
                <a href="https://arxiv.org/abs/2004.13297" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/face-manipulation.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>Towards Photo-Realistic Facial Expression Manipulation
                </strong></p>
                <p>Zhenglin Geng, Chen Cao, Sergey Tulyakov </p>
                <p><em>International Journal of Computer Vision, 2020</em></p>
                <a href="https://link.springer.com/article/10.1007/s11263-020-01361-8" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/human-motion-transfer.gif" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Human Motion Transfer from Poses in the Wild</p>
                </strong></p>
                <p>Jian Ren, Menglei Chai, Sergey Tulyakov, Chen Fang, Xiaohui Shen, Jianchao Yang</p>
                <p><em>Image Manipulation Workshop, Computer Vision and Pattern Recognition, 2020</em></p>
                <a href="https://arxiv.org/abs/2004.03142" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>


    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/motion-supervised.gif" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Motion-supervised Co-part Segmentation</p>
                </strong></p>
                <p>Aliaksandr Siarohin, Stéphane Lathuilière, Subhankar Roy, Sergey Tulyakov, Elisa Ricci and Nicu Sebe</p>
                <p><em>International Conference on Pattern Recognition, 2020</em></p>
                <a href="https://arxiv.org/abs/2004.03234" class="btn btn-primary">Paper</a>
                <a href="https://github.com/AliaksandrSiarohin/motion-cosegmentation" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>


    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/first-order-motion-model.gif" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>First Order Motion Model for Image Animation</p>
                </strong></p>
                <p>Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci and Nicu Sebe</p>
                <p><em>Neural Information Processing Systems, 2019</em></p>
                <a href="https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation" class="btn btn-primary">Paper</a>
                <a href="https://github.com/AliaksandrSiarohin/first-order-model" class="btn btn-primary">Code</a>
                <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/" class="btn btn-primary">Project</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/tbn.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Oral] Transformable Bottleneck Networks</p>
                </strong></p>
                <p>Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li, Linjie Luo</p>
                <p><em>International Conference on Computer Vision, 2019</em></p>
                <a href="https://arxiv.org/abs/1904.06458" class="btn btn-primary">Paper</a>
                <a href="https://github.com/kyleolsz/TB-Networks" class="btn btn-primary">Code</a>
                <a href="https://kyleolsz.github.io/TB-Networks/" class="btn btn-primary">Project</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/laplace.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Laplace Landmark Localization</p>
                </strong></p>
                <p>Joseph P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, and Sergey Tulyakov</p>
                <p><em>International Conference on Computer Vision, 2019</em></p>
                <a href="https://arxiv.org/abs/1903.11633" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>


    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/monkey-net.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Oral] Animating arbitrary objects via deep motion transfer</p>
                </strong></p>
                <p>Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe</p>
                <p><em>Computer Vision and Pattern Recognition, 2019</em></p>
                <a href="https://arxiv.org/abs/1812.08861" class="btn btn-primary">Paper</a>
                <a href="https://github.com/AliaksandrSiarohin/monkey-net" class="btn btn-primary">Code</a>
                <a href="http://www.stulyakov.com/papers/monkey-net.html" class="btn btn-primary">Project</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/3d-guided.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>3D Guided Fine-Grained Face Manipulation</p>
                </strong></p>
                <p>Zhenglin Geng, Chen Cao, Sergey Tulyakov</p>
                <p><em>Computer Vision and Pattern Recognition, 2019</em></p>
                <a href="https://arxiv.org/abs/1902.08900" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

 <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/expressive.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Real-Time Patch-Based Stylization of Portraits Using Generative Adversarial Network</p>
                </strong></p>
                <p>David Futschik, Menglei Chai, Chen Cao, Chongyang Ma, Aleksei Stoliar, Sergey Korolev, Sergey Tulyakov, Michal Kučera, and Daniel Sýkora</p>
                <p><em>ACM/EG Expressive Symposium, 2019</em></p>
                <a href="https://dcgi.fel.cvut.cz/home/sykorad/Futschik19-NPAR.pdf" class="btn btn-primary">Paper</a>
                <a href="https://dcgi.fel.cvut.cz/home/sykorad/facestyleGAN.html" class="btn btn-primary">Project</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/train-one.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Train One Get One Free: Partially Supervised Neural Network for Bug Report Duplicate Detection and Clustering</p>
                </strong></p>
                <p>Lahari Poddar, Leonardo Neves, William Brendel, Luis Marujo, Sergey Tulyakov, Pradeep Karuturi</p>
                <p><em>North American Chapter of the Association for Computational Linguistics, 2019</em></p>
                <a href="https://arxiv.org/abs/1903.12431" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>


    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/mocogan.png" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>MoCoGAN: Decomposing Motion and Content for Video Generation</p>
                </strong></p>
                <p>Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz</p>
                <p><em>Computer Vision and Pattern Recognition, 2018</em></p>
                <a href="https://arxiv.org/abs/1707.04993" class="btn btn-primary">Paper</a>
                <a href="https://github.com/sergeytulyakov/mocogan" class="btn btn-primary">Code</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/pami-wei.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Recurrent Convoltutional Shape Regression</p>
                </strong></p>
                <p>Wei Wang, Sergey Tulyakov and Nicu Sebe </p>
                <p><em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018</em></p>
                <a href="data/recurrent-convolutional-shape.pdf"
                   class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/hybrid-vae.png" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Hybrid VAE: Improving Deep Generative Models using Partial Observations</p>
                </strong></p>
                <p>Sergey Tulyakov, Andrew Fitzgibbon, Sebastian Nowozin</p>
                <p><em>Neural Information Processing Systems Workshops, 2017</em></p>
                <a href="https://arxiv.org/abs/1711.11566" class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/pami.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>Viewpoint-consistent 3D Face Alignment</p>
                </strong></p>
                <p>Sergey Tulyakov, Laszlo A. Jeni, Jeffrey F. Cohn and Nicu Sebe </p>
                <p><em>IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017</em></p>
                <a href="data/viewpoint-consistent-3d-face-alignment.pdf"
                   class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/accv-2016.png" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                <p>[Oral] Recurrent Convolutional Face Alignment</p>
                </strong></p>
                <p>Wei Wang, Sergey Tulyakov and Nicu Sebe </p>
                <p><em>Asian Conference on Computer Vision, 2016</em></p>
                <a href="data/recurrent-convolutional-face-alignment.pdf"
                   class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/cvpr-2016.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                    <b>[Oral] </b><a href="papers/cvpr2016.html">Self-Adaptive Matrix Completion for Heart Rate
                    Estimation from Face Videos under Realistic Conditions</a>
                </strong></p>
                <p> Sergey Tulyakov, Xavier Alameda Pineda, Elisa Ricci, Jijun Yin, Jeffrey Cohn and Nicu Sebe </p>
                <p><em>Computer Vision and Pattern Recognition, 2016</em></p>
                <a href="papers/cvpr2016.html" class="btn btn-primary">Read more</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/coming-soon.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                    <a href="papers/iccv2015.html">Regressing a 3D Face Shape from a Single Image</a>
                </strong></p>
                <p> Sergey Tulyakov and Nicu Sebe </p>
                <p><em>International Conference on Computer Vision, 2015</em></p>
                <a href="papers/iccv2015.html" class="btn btn-primary">Read more</a>
            </div>
        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/facecept3d.png" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                    FaceCept3D: Real Time 3D Face Tracking and Analysis
                </strong></p>
                <p> Sergey Tulyakov, Radu Vieriu, Enver Sangineto and Nicu Sebe </p>
                <p><em>International Conference on Computer Vision Workshops, 2015</em></p>
                <a href="data/FaceCept3D.%20Real%20Time%203D%20Face%20Tracking%20and%20Analysis.pdf"
                   class="btn btn-primary">Paper
                </a>
            </div>

        </div>
    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/fg-2015.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">

            <div>
                <p><strong>
                    Facial Expression Recognition under a Wide Range of Head Poses
                </strong></p>
                <p> Radu Vieriu, Sergey Tulyakov, Stas Semeniuta, Enver Sangineto and Nicu Sebe</p>

                <p><em>Automatic Face and Gesture Recognition, 2015</em></p>

                <a href="https://github.com/sergeytulyakov/FaceCept3D" class="btn btn-primary">Code</a>
                <a href="https://github.com/sergeytulyakov/FaceCept3D/raw/master/papers/Facial%20Expression%20Recognition%20under%20a%20Wide%20Range%20of%20Head%20Poses.pdf"
                   class="btn btn-primary">Paper</a>
            </div>
        </div>
    </div>
    <div class="row">

    </div>
    <hr/>

    <div class="row">
        <div class="col-md-3 col-sm-3">
            <img class="img-responsive " src="images/icpr-2014.jpg" alt=""/>
        </div>

        <div class="col-md-8 col-sm-8">
            <div>
                <p><strong>
                    Robust Real-Time Extreme Head Pose Estimation
                </strong></p>
                <p> Sergey Tulyakov, Radu Vieriu, Stas Semeniuta and Nicu Sebe</p>

                <p><em>International Conference on Pattern Recognition, 2014</em></p>

                <a href="https://github.com/sergeytulyakov/FaceCept3D" class="btn btn-primary">Code</a>
                <a href="#" class="btn btn-primary">Data</a>
                <a href="https://github.com/sergeytulyakov/FaceCept3D/raw/master/papers/Robust%20Real-Time%20Extreme%20Head%20Pose%20Estimation.pdf"
                   class="btn btn-primary">Paper</a>

            </div>
        </div>
    </div>

    <!-- Footer -->
    <hr>
    <footer>
        <div class="row">
            <div class="col-lg-12">
                <p>Sergey Tulyakov, September 2022</p>
            </div>
        </div>
    </footer>

</div>

</body>


</html>

